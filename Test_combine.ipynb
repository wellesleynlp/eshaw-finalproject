{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eshaw/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import csv, os, nltk, re, string, collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonrpc\n",
    "from simplejson import loads\n",
    "from scipy import stats\n",
    "from sklearn import cluster\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pca_tfidf(components=0):\n",
    "\tlist_of_characters_csv=[]\n",
    "\ttoken_dict = collections.defaultdict(list)\n",
    "\tsimilarity_dict = collections.defaultdict(list)\n",
    "\n",
    "\tfor subdir, dirs, files in os.walk('/home/eshaw/Documents/NLP/eshaw2-finalproject/test/csv'):\n",
    "\t    print 'Number of files present: ' + str(len(files))\n",
    "\t    for file in files:\n",
    "\t        list_of_characters_csv.append(file[0:-4])\n",
    "\t        file_path = subdir + os.path.sep + file\n",
    "\t        shakes = open(file_path, 'r')\n",
    "\t        text = shakes.read()\n",
    "\t        token_dict[file] = text\n",
    "\n",
    "\ttf = TfidfVectorizer(analyzer='word', min_df = 0)\n",
    "\ttfidf_matrix =  tf.fit_transform(token_dict.values())\n",
    "\n",
    "\tif (components == 0):\n",
    "\t\tpca = PCA()\n",
    "\t\tpca_tfidf = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\t\tsig_components = [x for x in pca.explained_variance_ if x > .005]\n",
    "\n",
    "\t\tpca = PCA(n_components = len(sig_components))\n",
    "\t\treturn list_of_characters_csv, pca.fit_transform(tfidf_matrix.toarray())\n",
    "\telse:\n",
    "\t\tpca = PCA(n_components = components)\n",
    "\t\treturn list_of_characters_csv, pca.fit_transform(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_char_grammar():\n",
    "\tlist_of_characters_txt=[]\n",
    "\tcharact_vocab = collections.defaultdict(list)\n",
    "\ttagged_vocab = collections.defaultdict(list)\n",
    "\tcharact_sents = collections.defaultdict(list)\n",
    "\tcharact_verbs = collections.defaultdict(list)\n",
    "\tcharact_attrs = collections.defaultdict(list)\n",
    "\tcharact_mods = collections.defaultdict(list)\n",
    "\tattrsets = collections.defaultdict(list)\n",
    "\tmodsets = collections.defaultdict(list)\n",
    "\tverbsets = collections.defaultdict(list)\n",
    "\n",
    "\tfor subdir, dirs, files in os.walk('/home/eshaw/Documents/NLP/eshaw2-finalproject/test/txt'):\n",
    "\t    # print 'Number of files present: ' + str(len(files))\n",
    "\t    for file in files:\n",
    "\t        character_name = file[0:-4]\n",
    "\t        list_of_characters_txt.append(character_name)\n",
    "\t        file_path = subdir + os.path.sep + file\n",
    "\t        shakes = open(file_path, 'r')\n",
    "\t        corpus = shakes.read()\n",
    "\t        charact_vocab[character_name] = corpus.split(\" \")\n",
    "\t        tagged_vocab[character_name] = nltk.pos_tag(corpus.split(\" \"))\n",
    "\t        charact_sents[character_name] = re.split('[?!;.:]',corpus)\n",
    "\t        del charact_sents[character_name][-1]\n",
    "\t        \n",
    "\t        verbs = []\n",
    "\t        attrs = []\n",
    "        \tmods = []\n",
    "\t        for pair in tagged_vocab[character_name]:\n",
    "\t            word, tag = pair\n",
    "\t            if re.search('V', tag):\n",
    "\t                verbs.append(word)\n",
    "\t            if re.search('JJ', tag) or re.search('NN', tag):\n",
    "                \tattrs.append(word)\n",
    "\t            if re.search('JJ', tag) or re.search('RB', tag):\n",
    "\t                mods.append(word)\n",
    "\t        charact_verbs[character_name] = verbs\n",
    "\t        verbsets[character_name] = set(verbs)\n",
    "\t        charact_attrs[character_name] = attrs\n",
    "\t        attrsets[character_name] = set(attrs)\n",
    "\t        charact_mods[character_name] = mods\n",
    "\t        modsets[character_name] = set(mods)\n",
    "\treturn list_of_characters_txt, charact_sents, verbsets, attrsets, modsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_grammar(dbug=False):\n",
    "    server = jsonrpc.ServerProxy(jsonrpc.JsonRpc20(),jsonrpc.TransportTcpIp(addr=(\"127.0.0.1\", 8080)))\n",
    "    list_of_characters_txt, charact_sents, verbsets, attrsets, modsets = get_char_grammar()\n",
    "\n",
    "    verb_types = collections.defaultdict(list)\n",
    "    agent_tags = set(['nsubj','agent'])\n",
    "    patient_tags = set(['dobj','nsubjpass','iobj'])\n",
    "    mod_types = collections.defaultdict(list)\n",
    "    attr_tags = set(['nsubj','appos','amod','nn'])\n",
    "    mod_tags = set(['acomp','advmod','prepc','tmod','xcomp'])\n",
    "\n",
    "    char_dep = np.zeros((len(list_of_characters_txt),4))\n",
    "    for c,character in enumerate(list_of_characters_txt):\n",
    "        if dbug:\n",
    "            print character\n",
    "\n",
    "        cur_verbset = verbsets[character]\n",
    "        cur_attrset = attrsets[character]\n",
    "        cur_modset = modsets[character]\n",
    "        for s, sentence in enumerate(charact_sents[character]):\n",
    "#             print s\n",
    "            if len(sentence) == 0:\n",
    "                continue\n",
    "            elif sentence[0] in set(string.punctuation):\n",
    "                sentence = sentence[2:]\n",
    "            elif ' - ' in sentence:\n",
    "                sentence = sentence.replace(' - ','')\n",
    "                \n",
    "            try:\n",
    "                result = loads(server.parse(sentence))\n",
    "            except:\n",
    "                failed_sent = (character, str(s), sentence)\n",
    "                txtfile = open(\"/home/eshaw/Documents/NLP/eshaw2-finalproject/failed_sentences.txt\", \"wb\")\n",
    "                txtfile.write(' '.join(failed_sent))\n",
    "                break\n",
    "\n",
    "            for relation in result['sentences'][0]['dependencies']:\n",
    "\n",
    "                if (relation[1] in cur_verbset) or (relation[2] in cur_verbset):\n",
    "                    if relation[0] in agent_tags:\n",
    "                        char_dep[c,0] += 1\n",
    "                    elif (relation[0] in patient_tags) or ('prep_' in relation[0]):\n",
    "                        char_dep[c,1] += 1\n",
    "\n",
    "                elif (relation[1] in cur_attrset) or (relation[2] in cur_attrset):\n",
    "                    if relation[0] in attr_tags:\n",
    "                        char_dep[c,2] += 1\n",
    "\n",
    "                elif (relation[1] in cur_modset) or (relation[2] in cur_modset):\n",
    "                    if relation[0] in mod_tags:\n",
    "                        char_dep[c,3] += 1\n",
    "\n",
    "    return list_of_characters_txt, char_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_context_matrix(A,B):\n",
    "\tagent = stats.zscore(B[:,0])\n",
    "\tpatient = stats.zscore(B[:,1])\n",
    "\tattr = stats.zscore(B[:,2])\n",
    "\tmod = stats.zscore(B[:,3])\n",
    "\tgrammar_matrix = np.column_stack((agent,patient,attr,mod))\n",
    "\treturn np.concatenate((A,grammar_matrix), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clustering(num, matrix, query=True, characters=[]):\n",
    "    pylab.rcParams['figure.figsize'] = (12, 10)\n",
    "\n",
    "    if query:\n",
    "        n = num\n",
    "        inertias = []\n",
    "\n",
    "        for i in range(n):\n",
    "            centroids,labels,inertia = cluster.k_means(matrix,n_clusters=i+1)\n",
    "            inertias.append(inertia)\n",
    "\n",
    "        #find fit\n",
    "        coefficients = np.polyfit(range(n), inertias, 6)\n",
    "        polynomial = np.poly1d(coefficients)\n",
    "        fit_y = polynomial(range(n))\n",
    "\n",
    "        #Absolute change in inertia\n",
    "        delta_yfit = numpy.diff(fit_y)\n",
    "        plt.plot(range(1,len(delta_yfit)+1), delta_yfit)\n",
    "        plt.xlabel('Nth cluster',fontsize=14)\n",
    "        plt.ylabel('inertia change',fontsize=14)\n",
    "        plt.title('Inertia changes for various number of clusters',fontsize=18)\n",
    "        plt.show()\n",
    "\n",
    "        # Relative change in inertia\n",
    "        for i in range(n-1):\n",
    "            print [i,i+1,abs(np.round(delta_yfit[i]/fit_y[i],2))]\n",
    "\n",
    "        plt.plot(range(1,len(delta_yfit)+1), abs(delta_yfit/fit_y[0:num-1]))\n",
    "        plt.xlabel('Nth cluster',fontsize=14)\n",
    "        plt.ylabel('% change in inertia',fontsize=14)\n",
    "        plt.title('abs(% of Inertia change) for various number of clusters',fontsize=18)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        cluster_func = cluster.KMeans(n_clusters=num, n_init =20)\n",
    "        cluster_func.fit(matrix)\n",
    "        assignments = cluster_func.labels_\n",
    "\n",
    "        # find overall distribution\n",
    "        counts = np.zeros(num)\n",
    "        for i in range(len(assignments)):\n",
    "            counts[assignments[i]] +=1\n",
    "        overallDist = np.round(np.divide(counts, float(sum(counts))),4)\n",
    "\n",
    "        # make dict out of cluster -> characters\n",
    "        cluster_assign = collections.defaultdict(list)\n",
    "        for ith in range(0,num):\n",
    "            cluster_assign[ith] = [characters[i] for i in np.where(cluster_func.labels_==ith)[0]]\n",
    "\n",
    "        # make dict out of characters -> clusters\n",
    "        charact_assign = dict(zip(characters, cluster_func.labels_))\n",
    "\n",
    "        return overallDist, charact_assign, cluster_assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation(characterlist, cosine_matrix, char_assign):\n",
    "    char_ind = collections.defaultdict(list)\n",
    "    for c,character in enumerate(characterlist):\n",
    "        char_ind[character]=c\n",
    "\n",
    "    top3 = collections.defaultdict(list)\n",
    "    for c,character in enumerate(characterlist):\n",
    "        similarity[c] = [val if val < similarity[c].max() else 0.0 for val in similarity[c]]\n",
    "        magnitude = [abs(val) for val in similarity[c]]\n",
    "        top3_mag = sorted(zip(magnitude, characterlist), reverse=True)[:3]\n",
    "        top3_names = [name for val, name in top3_mag]\n",
    "        vals = []\n",
    "        for name in top3_names:\n",
    "            vals.append(similarity[c][char_ind[name]])\n",
    "                \n",
    "        sim3 = sorted(zip(similarity[c], characterlist), reverse=True)[:3]\n",
    "        op3 = sorted(zip(similarity[c], characterlist), reverse=False)[:3]\n",
    "        top3_list = []\n",
    "        top3_list.append(zip(vals, top3_names))\n",
    "        top3_list.append(sim3)\n",
    "        top3_list.append(op3)\n",
    "        top3[character] = [item for sublist in top3_list for item in sublist]\n",
    "\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for pair in top3[character]:\n",
    "        if abs(pair[0]) > .5:\n",
    "            if char_assign[pair[1]] == char_assign[character]:\n",
    "                correct +=1\n",
    "            else:\n",
    "                incorrect +=1\n",
    "\n",
    "    return float(correct)/(correct + incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files present: 7\n",
      "I now have PCA on TF-IDF.\n",
      "sicilius leonatus (cymbelin)\n",
      "lord (win_tale)\n",
      "nestor (troilus)\n",
      "queen margaret (rich_iii)\n",
      "aemelia (com_err)\n",
      "curtis (taming)\n",
      "costard (lll)\n",
      "I now have Grammar."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['loads']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Based on inertia change, how many clusters would you like?2\n"
     ]
    }
   ],
   "source": [
    "characterlistcsv, tfidf = get_pca_tfidf()\n",
    "print 'I now have PCA on TF-IDF.'\n",
    "characterlisttxt, grammar = get_grammar(True)\n",
    "print 'I now have Grammar.'\n",
    "\n",
    "reordered_gram = np.zeros((len(characterlistcsv),4))\n",
    "for n,name in enumerate(characterlistcsv):\n",
    "    grammar_ind = characterlisttxt.index(name)\n",
    "    reordered_gram[n,:] = grammar[grammar_ind,:]\n",
    "context_matrix = get_context_matrix(tfidf, reordered_gram)\n",
    "similarity = cosine_similarity(context_matrix)\n",
    "\n",
    "%pylab inline\n",
    "clustering(2, context_matrix, query=False, characters= characterlistcsv)\n",
    "\n",
    "requested_clusters = input('Based on inertia change, how many clusters would you like?')\n",
    "distribution, char_assign, cluster_assign = clustering(requested_clusters, context_matrix, query=False, characters= characterlistcsv)\n",
    "accuracy = validation(characterlistcsv, similarity, char_assign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    characterlistcsv, tfidf = get_pca_tfidf()\n",
    "    print 'I now have PCA on TF-IDF.'\n",
    "    characterlisttxt, grammar = get_grammar(True)\n",
    "    print 'I now have Grammar.'\n",
    "    \n",
    "    reordered_gram = np.zeros((len(characterlistcsv),4))\n",
    "    for n,name in enumerate(characterlistcsv):\n",
    "        grammar_ind = characterlisttxt.index(name)\n",
    "        reordered_gram[n,:] = grammar[grammar_ind,:]\n",
    "    context_matrix = get_context_matrix(tfidf, reordered_gram)\n",
    "    similarity = cosine_similarity(context_matrix)\n",
    "    \n",
    "    %pylab inline\n",
    "    clustering(2, context_matrix, query=False, characters= characterlistcsv)\n",
    "    \n",
    "    requested_clusters = input('Based on inertia change, how many clusters would you like?')\n",
    "    distribution, char_assign, cluster_assign = clustering(requested_clusters, context_matrix, query=False, characters= characterlistcsv)\n",
    "    return characterlistcsv, similarity, distribution, char_assign\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "characterlist, similarity, dist, assign= main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_ind = collections.defaultdict(list)\n",
    "for c,character in enumerate(characterlist):\n",
    "    char_ind[character]=c\n",
    "\n",
    "top3 = collections.defaultdict(list)\n",
    "sim3 = collections.defaultdict(list)\n",
    "op3 = collections.defaultdict(list)\n",
    "for c,character in enumerate(characterlist):\n",
    "#     similarity[c] = [val if val < similarity[c].max() else 0.0 for val in similarity[c]]\n",
    "    magnitude = [abs(val) for val in similarity[c]]\n",
    "    top3_mag = sorted(zip(magnitude, characterlist), reverse=True)[:3]\n",
    "    top3_names = [name for val, name in top3_mag]\n",
    "    vals = []\n",
    "    for name in top3_names:\n",
    "        vals.append(similarity[c][char_ind[name]])\n",
    "        \n",
    "    sim3 = sorted(zip(similarity[c], characterlist), reverse=True)[:3]\n",
    "    op3 = sorted(zip(similarity[c], characterlist), reverse=False)[:3]\n",
    "    top3_list = []\n",
    "    top3_list.append(zip(top3_names, vals))\n",
    "    top3_list.append(sim3)\n",
    "    top3_list.append(op3)\n",
    "    top3[character] = [item for sublist in top3_list for item in sublist]\n",
    "\n",
    "print top3['aemelia (com_err)']\n",
    "# print sim3['aemelia (com_err)']\n",
    "# print op3['aemelia (com_err)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = validation(characterlist, cosine_similarity(context_matrix), char_assign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosine_similarity(context_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "characterlistcsv, tfidf = get_pca_tfidf()\n",
    "print 'I now have PCA on TF-IDF.'\n",
    "characterlisttxt, grammar = get_grammar(True)\n",
    "print 'I now have Grammar.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reordered_gram = np.zeros((len(characterlistcsv),4))\n",
    "for n,name in enumerate(characterlistcsv):\n",
    "    grammar_ind = characterlisttxt.index(name)\n",
    "#     print  grammar[grammar_ind,:]\n",
    "    reordered_gram[n,:] = grammar[grammar_ind,:]\n",
    "\n",
    "print reordered_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn import cluster\n",
    "%pylab inline\n",
    "clustering(len(characterlistcsv), context_matrix, query=True, characters= characterlistcsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "requested_clusters = input('Based on inertia change, how many clusters would you like? \\n')\n",
    "clustering(requested_clusters, context_matrix, query=False, characters= characterlistcsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
